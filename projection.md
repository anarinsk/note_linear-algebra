

# Meaning of Projection

![](http://blogs.jccc.edu/rgrondahl/files/2012/02/perpendicularprojection.jpg)

$u \cdot v = \lVert \text{Proj}_v u \rVert \lVert v \rVert$

$\cos \theta$ is to be $\dfrac{\lVert \text{Proj}_v u \rVert}{\lVert u \rVert}$. 

Thus, 

$$
\cos \theta = \frac{u \cdot v}{\lVert u \rVert {\,}\lVert v \rVert}
$$

# Dot product with COS 

## How to proof 

[LINK](https://math.stackexchange.com/questions/116133/how-to-understand-dot-product-is-the-angles-cosine)

* [Law of COS](https://en.wikipedia.org/wiki/Law_of_cosines)

$$
\lVert a - b \rVert^2 = \lVert a \rVert^2 + \lVert  b \rVert^2 - 2\lVert a \rVert \lVert  b \rVert \cos \theta
$$

* [bilinearity](https://en.wikipedia.org/wiki/Bilinear_map) and symmetry 

$$
\lVert a - b \rVert^2 = \langle a-b, a-b \rangle = \lVert a \rVert^2 +  \lVert b \rVert^2 - 2\langle a, b \rangle 
$$

Thus, 

$$
\langle a, b \rangle = \lvert\lvert a \rvert\rvert  {\,} \lvert\lvert b \rvert\rvert \cos \theta 
$$

Geometrically, dot product is multiplication of the length of vector projected and the one projecting. 

# Cauchy-Schwarz Inequality 

## Statement 

$$
\rvert \langle u, v \rangle \lvert^2 \leq \langle v, v \rangle \langle u,u \rangle
$$ 

## Proof 

if $v=0$, equality holds. 

Let 

$$
z = u - u_v = u - \frac{\langle u,v \rangle}{\langle v,v \rangle} v
$$

$\langle z, v \rangle = 0$ holds. That is, $z$ and $v$ are orthogonal. 

$$
\lVert u \rVert^2 = \bigg\lvert \dfrac{\langle u,v \rangle}{\langle v, v \rangle} \bigg\rvert^2 \lVert v \rVert^2 + \lVert z\rVert^2 =   \dfrac{\lvert \langle u,v \rangle \rvert^2}{\lVert v \rVert^2} + \lVert z \rVert^2 \geq \dfrac{\lvert \langle u,v \rangle \rvert^2}{\lVert v \rVert^2} .
$$

# Regression coefficient 

![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/OLS_geometric_interpretation.svg/1280px-OLS_geometric_interpretation.svg.png)


Meaning of orthogonal projection: [LINK](
https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/projections-onto-subspaces/MIT18_06SCF11_Ses2.2sum.pdf)

- $y$: $n \times 1$ vector of explained data 
- $X$: column space generated by $k$ $n \times 1$ vectors

In this view, regression is a project from $y$ onto column space of $X$. Column space is spanned by $\left[ x_1, \dotsc, x_k \right]$ where $x_i$ is 


Thus, 

$$
\left(y-X \hat\beta \right)^{\text T} (X \hat{\beta})= 0 
$$

By some calculation, $\beta = (X^{\text T} X)^{-1}X^{\text T} y$



<!--stackedit_data:
eyJoaXN0b3J5IjpbMTYwODg2MTE4MywtNTE4OTE4ODUzLDYxNj
E2NjU0MCwyMDk2NzU4MTEsMzgwNTIzMjE1LDE0NzU0NTQ3Njcs
MTM2MDk2MzU2NSw2ODUwMjU0NTYsOTM2NzI4ODIsLTE3MzkzMj
ExNzMsLTIwNTMwNjgxNjcsNTYxNTE3NzMxLC0xNzc1NTYzOTYy
XX0=
-->