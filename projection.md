

# Meaning of Projection 

![](http://blogs.jccc.edu/rgrondahl/files/2012/02/perpendicularprojection.jpg)

$u \cdot v = \lVert \text{Proj}_v u \rVert \lVert v \rVert$

$\cos \theta$ is to be $\dfrac{\lVert \text{Proj}_v u \rVert}{\lVert u \rVert}$. 

Thus, 

$$
\cos \theta = \frac{u \cdot v}{\lVert u \rVert {\,}\lVert v \rVert}
$$

# Dot product with COS 

## How to proof 

[LINK](https://math.stackexchange.com/questions/116133/how-to-understand-dot-product-is-the-angles-cosine)

* [Law of COS](https://en.wikipedia.org/wiki/Law_of_cosines)

$$
\lVert a - b \rVert^2 = \lVert a \rVert^2 + \lVert  b \rVert^2 - 2\lVert a \rVert \lVert  b \rVert \cos \theta
$$

* [bilinearity](https://en.wikipedia.org/wiki/Bilinear_map) and symmetry 

$$
\lVert a - b \rVert^2 = \langle a-b, a-b \rangle = \lVert a \rVert^2 +  \lVert b \rVert^2 - 2\langle a, b \rangle 
$$

Thus, 

$$
\langle a, b \rangle = \lvert\lvert a \rvert\rvert  {\,} \lvert\lvert b \rvert\rvert \cos \theta 
$$

Geometrically, dot product is multiplication of the length of vector projected and the one projecting. 

# Cauchy-Schwarz Inequality 

## Statement 

$$
\rvert \langle u, v \rangle \lvert^2 \leq \langle v, v \rangle \langle u,u \rangle
$$ 

## Proof 

if $v=0$, equality holds. 

Let 

$$
z = u - u_v = u - \frac{\langle u,v \rangle}{\langle v,v \rangle} v
$$

$\langle z, v \rangle = 0$ holds. That is, $z$ and $v$ are orthogonal. 

$$
\lVert u \rVert^2 = \bigg\lvert \dfrac{\langle u,v \rangle}{\langle v, v \rangle} \bigg\rvert^2 \lVert v \rVert^2 + \lVert z\rVert^2 =   \dfrac{\lvert \langle u,v \rangle \rvert^2}{\lVert v \rVert^2} + \lVert z \rVert^2 \geq \dfrac{\lvert \langle u,v \rangle \rvert^2}{\lVert v \rVert^2} .
$$

# Regression coefficient 

![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/OLS_geometric_interpretation.svg/1280px-OLS_geometric_interpretation.svg.png)


Meaning of orthogonal projection: [LINK](
https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/projections-onto-subspaces/MIT18_06SCF11_Ses2.2sum.pdf)

Think that vector $\hat \varepsilon$ is orthogonal to plane. Thus, any point in the plane is orthogonal to $\hat \varepsilon$.

- $y$: $n \times 1$ vector of explained data 
- $X$: column space generated by $k$ of $n \times 1$ vectors

In this view, regression is a project from $y$ onto column space of $X$. Column space is spanned by $\left[ x_1, \dotsc, x_k \right]$ where $x_i$ is $i$'th column vector in the basis. 

$$
\begin{aligned}
\hat \beta_1 x_1^{\text T} (y- X \hat \beta) & = 0 \\
& \vdots  \\
\hat \beta_k x_k^{\text T} (y- X \hat \beta) & = 0 
\end{aligned}
$$

Thus, in matrix form, 

$$
\left(y-X \hat\beta \right)^{\text T} X= \mathbf 0 
$$

By some calculation, $\beta = (X^{\text T} X)^{-1}X^{\text T} y$


# Distance from a vector point to a plane 

https://mathinsight.org/distance_point_plane

## Definition of problem 

- A plane: $a x +  b y + c z + d =0$
- A point outside the plane $(x_1, y_1, z_1)$
- A point on the plane: $(x_0, y_0, z_0)$

Argument 1
$a(x-x_0) + b(y-y_0) + c(z-z_0) = 0$ where $d = -(a x_0 + b y_0 + c z_0)$

Argument 2
Normal of plane is $(a, b, c)$ 

proof
http://tutorial.math.lamar.edu/Classes/CalcIII/EqnsOfPlanes.aspx

Final argument 
https://mathinsight.org/distance_point_plane



<!--stackedit_data:
eyJoaXN0b3J5IjpbMTg0NjQ3NTk3Miw3OTI2MzYyOTgsLTE2OD
U3NzU3MDQsLTEyOTgxMDYzMDcsODc1MDAyOTQsMTg5MDE0ODQy
OCwtNTE4OTE4ODUzLDYxNjE2NjU0MCwyMDk2NzU4MTEsMzgwNT
IzMjE1LDE0NzU0NTQ3NjcsMTM2MDk2MzU2NSw2ODUwMjU0NTYs
OTM2NzI4ODIsLTE3MzkzMjExNzMsLTIwNTMwNjgxNjcsNTYxNT
E3NzMxLC0xNzc1NTYzOTYyXX0=
-->